è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ç­–ç•¥ã€‚ç»™ AI æä¾›ä¸€ä»½ç‹¬ç«‹çš„**æŠ€æœ¯å‚è€ƒæ–‡æ¡£ (Reference Manual)**ï¼Œå¯ä»¥è®©å®ƒæ›´å‡†ç¡®åœ°ç†è§£â€œSVD çº¦æŸâ€å…·ä½“æ˜¯æŒ‡ä»€ä¹ˆæ•°å­¦æ“ä½œå’Œä»£ç é€»è¾‘ï¼Œè€Œä¸éœ€è¦ä¾èµ–é‚£ä¸ªå¤æ‚çš„æ—§é¡¹ç›®ã€‚

è¿™é‡Œä¸ºæ‚¨å‡†å¤‡äº†ä¸¤éƒ¨åˆ†å†…å®¹ï¼š

1. **`SVD_Implementation_Reference.md`**ï¼šä¸€ä»½è¯¦ç»†çš„ç®—æ³•å®ç°å‚è€ƒæ–‡æ¡£ï¼Œæ‚¨å¯ä»¥ç›´æ¥å‘ç»™ AIã€‚
2. **é…å¥—çš„ Prompt**ï¼šå‘Šè¯‰ AI â€œè¯·é˜…è¯»è¿™ä»½å‚è€ƒæ–‡æ¡£ï¼Œå¹¶åœ¨ ForgeLens ä¸­å®ç°å®ƒâ€ã€‚

---

### ğŸ“„ ç¬¬ä¸€éƒ¨åˆ†ï¼šSVD æŠ€æœ¯å‚è€ƒæ–‡æ¡£ (è¯·ä¿å­˜ä¸º `.md` æ–‡ä»¶æˆ–ç›´æ¥å¤åˆ¶)

æ‚¨æŠŠä¸‹é¢è¿™æ®µå†…å®¹å¤åˆ¶ç»™ AIï¼Œå‘Šè¯‰å®ƒï¼šâ€œè¿™æ˜¯ç®—æ³•å®ç°çš„å‚è€ƒæ ‡å‡†â€ã€‚

```markdown
# Reference: SVD Orthogonal Constraint for CLIP Adaptation

## 1. ç®—æ³•èƒŒæ™¯ (Theoretical Background)

åœ¨å¯¹ CLIP æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰æ—¶ï¼Œä¸ºäº†é˜²æ­¢â€œç¾éš¾æ€§é—å¿˜â€ï¼ˆCatastrophic Forgettingï¼‰å¹¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹å¼ºå¤§çš„é€šç”¨è¯­ä¹‰èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥ **SVD æ­£äº¤çº¦æŸï¼ˆSVD Orthogonal Constraintï¼‰**ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
1.  **ç‰¹å¾å­ç©ºé—´æå–**ï¼šé€šè¿‡å¯¹å†»ç»“çš„ CLIP Vision Transformer çš„æ³¨æ„åŠ›æƒé‡è¿›è¡Œ SVD åˆ†è§£ï¼Œæå–å‡ºä¸»è¦çš„â€œè¯­ä¹‰ç‰¹å¾å­ç©ºé—´â€ï¼ˆPrincipal Semantic Subspaceï¼‰ã€‚
2.  **æ­£äº¤æƒ©ç½š**ï¼šå¼ºåˆ¶æ–°å¢çš„é€‚é…å™¨ï¼ˆAdapter/WSGMï¼‰ç”Ÿæˆçš„ç‰¹å¾å¢é‡ $\Delta x$ ä¸è¯¥å­ç©ºé—´ä¿æŒ**æ­£äº¤**ã€‚è¿™æ„å‘³ç€é€‚é…å™¨å­¦ä¹ çš„æ˜¯ CLIP å°šæœªæŒæ¡çš„â€œæ®‹å·®çŸ¥è¯†â€ï¼Œè€Œä¸æ˜¯è¦†ç›–å·²æœ‰çš„çŸ¥è¯†ã€‚

## 2. æ•°å­¦è¡¨è¿° (Mathematical Formulation)

å‡è®¾ Transformer æœ‰ $L$ å±‚ï¼Œæ¯å±‚çš„æ³¨æ„åŠ›è¾“å…¥æŠ•å½±çŸ©é˜µä¸º $W_{in\_proj}^{(l)} \in \mathbb{R}^{3D \times D}$ï¼ˆå…¶ä¸­ $D$ æ˜¯ç‰¹å¾ç»´åº¦ï¼‰ã€‚

1.  **æ„å»ºæƒé‡çŸ©é˜µ**ï¼šå°†æ‰€æœ‰å±‚çš„æŠ•å½±æƒé‡æ‹¼æ¥ï¼š
    $$W_{all} = \text{Concat}(W_{in\_proj}^{(1)}, \dots, W_{in\_proj}^{(L)}) \in \mathbb{R}^{(L \cdot 3D) \times D}$$

2.  **SVD åˆ†è§£**ï¼š
    $$W_{all} = U \Sigma V^T$$
    å…¶ä¸­ $V^T \in \mathbb{R}^{D \times D}$ çš„è¡Œå‘é‡ä»£è¡¨äº†è¾“å…¥ç‰¹å¾ç©ºé—´çš„ä¸»è¦å˜åŒ–æ–¹å‘ã€‚

3.  **æå–åŸºåº• (Basis)**ï¼š
    å– $V^T$ çš„å‰ $K$ ä¸ªè¡Œå‘é‡ä½œä¸ºå­ç©ºé—´åŸºåº• $B \in \mathbb{R}^{K \times D}$ã€‚
    $$B = V^T_{[0:K, :]}$$

4.  **æ­£äº¤æŸå¤± (Orthogonal Loss)**ï¼š
    å¯¹äºé€‚é…å™¨çš„è¾“å‡º $h_{adapter} \in \mathbb{R}^{B \times D}$ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒåœ¨è¯¥å­ç©ºé—´ä¸Šçš„æŠ•å½±å°½å¯èƒ½å°ï¼š
    $$P = h_{adapter} \cdot B^T$$
    $$L_{orth} = \| P \|_F^2 = \text{mean}(P^2)$$

## 3. PyTorch å®ç°å‚è€ƒ (Implementation Snippets)

### 3.1 SVD è®¡ç®—å‡½æ•° (Compute SVD)

æ­¤å‡½æ•°åº”ä½œä¸º `Transformer` ç±»çš„ä¸€ä¸ªæ–¹æ³•ã€‚å®ƒä»æ‰€æœ‰å±‚æ”¶é›†æƒé‡ï¼Œè®¡ç®—æŠ•å½±çŸ©é˜µï¼Œå¹¶æ³¨å†Œä¸ºä¸å¯è®­ç»ƒçš„ Bufferã€‚

```python
import torch

def compute_and_register_svd(self, rank=64):
    """
    Computes the semantic subspace basis from frozen CLIP weights using SVD.
    Registers the projection matrix to buffers.
    """
    print(f"Computing SVD with rank {rank}...")
    
    # 1. Collect weights from all ResidualAttentionBlocks
    # CLIP's Attention layer is usually named 'attn' and weights are 'in_proj_weight'
    weights = []
    for block in self.resblocks:
        weights.append(block.attn.in_proj_weight.data)  # Shape: [3*width, width]
    
    # 2. Concatenate all layers
    W_all = torch.cat(weights, dim=0) # Shape: [N_layers * 3 * width, width]
    
    # 3. Perform SVD
    # Note: torch.linalg.svd returns U, S, Vh. 
    # Vh (V hermitian) shape is [width, width], rows are eigenvectors.
    # We use float32 for stability.
    U, S, Vh = torch.linalg.svd(W_all.float(), full_matrices=False)
    
    # 4. Select top-K components
    # basis shape: [rank, width]
    basis = Vh[:rank, :]
    
    # 5. Create Projection Matrix: B.T
    # Shape: [width, rank]. Use this to project input x: (x @ proj_matrix)
    proj_matrix = basis.T.to(dtype=W_all.dtype) # Convert back to fp16 if needed
    
    # 6. Register to all blocks
    # We register it as a buffer so it's part of state_dict but not trainable
    for block in self.resblocks:
        if hasattr(block, 'register_buffer'):
             block.register_buffer('proj_matrix', proj_matrix)
        else:
             block.proj_matrix = proj_matrix

    print("SVD computation done. Projection matrix registered.")

```

### 3.2 åœ¨ Forward ä¸­è®¡ç®— Loss (Forward Pass)

åœ¨ `ResidualAttentionBlock.forward` ä¸­ï¼š

```python
def forward(self, x):
    # Standard Attention & Residual
    attn_out = self.attention(self.ln_1(x))
    x = x + attn_out
    
    orth_loss = 0.0
    
    # WSGM / Adapter Logic
    if self.use_wsgm and self.wsgm_module is not None:
        wsgm_out = self.wsgm_module(x)
        
        # --- SVD Constraint Implementation ---
        if hasattr(self, 'proj_matrix') and self.proj_matrix is not None:
            # Project adapter output onto the semantic subspace
            # wsgm_out: [L, N, D], proj_matrix: [D, Rank]
            projection = torch.matmul(wsgm_out, self.proj_matrix)
            
            # Calculate L2 norm squared (Energy)
            # IMPORTANT: Do NOT use .detach() here. We need gradients to suppress this component.
            orth_loss = torch.mean(projection ** 2) 
        # -------------------------------------
        
        x = x + wsgm_out

    # MLP & Residual
    x = x + self.mlp(self.ln_2(x))
    
    return x, orth_loss

```

## 4. å…³é”®æ³¨æ„äº‹é¡¹ (Best Practices)

1. **åˆå§‹åŒ–æ—¶æœº (Initialization Timing)**:
SVD è®¡ç®—**å¿…é¡»**åœ¨ `load_state_dict()` åŠ è½½å®Œé¢„è®­ç»ƒæƒé‡**ä¹‹å**è¿›è¡Œã€‚å¦‚æœåœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼ˆæƒé‡è¿˜æ˜¯éšæœºçš„ï¼‰å°±è®¡ç®—ï¼Œå¾—åˆ°çš„å­ç©ºé—´æ˜¯æ— æ„ä¹‰çš„å™ªå£°ã€‚
2. **æ¢¯åº¦ä¿ç•™ (Gradient Preservation)**:
åœ¨è®¡ç®— `orth_loss` æ—¶ï¼Œåˆ‡è®°**ä¸èƒ½**åˆ‡æ–­æ¢¯åº¦ï¼ˆä¸è¦ç”¨ `.detach()`ï¼‰ã€‚æŸå¤±å‡½æ•°çš„ç›®çš„æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™å‘Šè¯‰ WSGMï¼šâ€œä¸è¦å¾€è¿™ä¸ªæ–¹å‘ç”Ÿæˆç‰¹å¾â€ã€‚
3. **è¶…å‚æ•°å»ºè®® (Hyperparameters)**:
* `svd_rank`: æ¨è **64**ã€‚
* `orth_lambda`: æ¨è **0.1** (Total Loss = Cls Loss + 0.1 * Orth Loss)ã€‚
